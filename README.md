# AutoControl-Bench ğŸš—ğŸ§ 

**AutoControl-Bench: A Three-Tier Benchmark for Ambiguity-Rich Function Call Evaluation in Vehicle Language Understanding**

> Official benchmark release for our NeurIPS 2025 paper:
> **"AutoControl-Bench: A Multi-Agent Benchmark for Protocol-Compliant Vehicle Function Call Comprehension"**

---

## ğŸ” Overview

AutoControl-Bench is a function-call benchmark designed to evaluate large language models (LLMs) in realistic, high-ambiguity vehicle control scenarios. It introduces:

- ğŸ§© **9 Types of Linguistic Ambiguity**  
- ğŸ”„ **Multi-turn Dialogue Understanding**  
- âœ… **Protocol-compliant Function Call Constraints**  
- ğŸ¤– **Multi-Agent Benchmark Construction Pipeline**

---

## ğŸ“Š Key Features

| Feature                          | Description                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| Ambiguity Coverage               | 9 types (e.g., vague references, underspecification, ellipsis, etc.)       |
| Evaluation Protocol              | Three-tier: fuzzy parsing, counter-questioning, multi-turn consistency      |
| Benchmark Size                   | 30,000 examples (10k Tier-1, 10k Tier-2, 10k Tier-3)                        |
| Scenarios                        | Safety-critical, Entertainment, Autonomous Driving, Comfort                 |
| Output Format                    | JSON-based instructionâ€“response pairs with function call targets            |
| Agents Involved                  | Semantic Parsing, Adversarial Gen, Fuzz Injection, Multi-turn Simulation    |

---

## ğŸ“ Repository Structure

```bash
AutoControl-Bench/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ tier1_single_turn.json
â”‚   â”œâ”€â”€ tier2_fuzzy_clarify.json
â”‚   â”œâ”€â”€ tier3_multi_turn.json
â”‚   â””â”€â”€ meta/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generation_pipeline.py
â”‚   â””â”€â”€ eval_metrics.py
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ demo_queries.md
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ qwen2.5_results.json
â”œâ”€â”€ croissant_metadata.json
â””â”€â”€ README.md
âš™ï¸ Quick Start
1. Install dependencies
bash
å¤åˆ¶
ç¼–è¾‘
pip install -r requirements.txt
2. Download data
All data is in the data/ folder. To load the Tier-3 multi-turn benchmark:

python
å¤åˆ¶
ç¼–è¾‘
import json
with open('data/tier3_multi_turn.json') as f:
    samples = json.load(f)
3. Run Evaluation
bash
å¤åˆ¶
ç¼–è¾‘
python scripts/eval_metrics.py --input <predictions.json> --ref data/tier3_multi_turn.json
ğŸ“Œ Evaluation Metrics
Metric	Description
IRA	Intent Recognition Accuracy
PEP	Parameter Extraction Precision
FDR	Fuzzy Detection Rate
CQC	Counter-Question Coverage
DC	Dialogue Consistency (multi-turn)
FESR	Final Execution Success Rate

All metrics are implemented in scripts/eval_metrics.py.

ğŸ“„ Dataset Format (JSON)
Each sample in the benchmark includes:

json
å¤åˆ¶
ç¼–è¾‘
{
  "id": "multi_001",
  "tier": "Tier-3",
  "dialogue": [
    {"user": "Turn the lights on.", "assistant": "Which lights? Headlights or interior?"},
    {"user": "Headlights, please."}
  ],
  "target_call": {
    "function": "control_lighting",
    "parameters": {"zone": "headlights", "state": "on"}
  },
  "meta": {
    "ambiguity_type": "underspecification",
    "protocol_compliant": true
  }
}
ğŸ§ª Baseline Results
Model	DQS â†‘	FESR â†‘	CQC â†‘	DC â†‘
GPT-4	0.69	90.0	85.2	87.0
Claude 3	0.70	91.5	84.1	86.5
Qwen2.5-7B-SFT ğŸ¥‡	0.88	97.5	99.2	94.8

ğŸ“œ License
This project is released under the Apache 2.0 License.

ğŸ“¬ Citation
If you use AutoControl-Bench in your research, please cite:

bibtex
å¤åˆ¶
ç¼–è¾‘
@inproceedings{autocontrolbench2025,
  title     = {AutoControl-Bench: A Multi-Agent Benchmark for Protocol-Compliant Vehicle Function Call Comprehension},
  author    = {Anonymous et al.},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2025}
}
